{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4600, 57)\n",
      "(4600,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "X,y = load_spam()\n",
    "\n",
    "\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "print(\"\")\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "\n",
    "print(X.isnull().sum().sum())\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train Test split of the full dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, stratify= y ,random_state=0)\n",
    "\n",
    "# Train-Test split for the first two columns\n",
    "X_train_f2, X_val_f2, y_train_f2, y_val_f2 = train_test_split(X.iloc[:,0:2],y,random_state=0)\n",
    "\n",
    "# Train-Test split for the small set\n",
    "X_small,_,y_small,_ = train_test_split(X,y, train_size=0.05, random_state=0)\n",
    "X_small_train,X_small_val,y_small_train,y_small_val = train_test_split(X_small,y_small, random_state=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195bfb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model1 = LogisticRegression(max_iter=2000)\n",
    "model2 = LogisticRegression(max_iter=2000)\n",
    "model3 = LogisticRegression(max_iter=2000)\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "\n",
    "model2.fit(X_train_f2,y_train_f2)\n",
    "\n",
    "model3.fit(X_small_train,y_small_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efb0f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for X & y dataset: 0.9342\n",
      "Validation set score for X & y dataset: 0.9313\n",
      "\n",
      "Training set score for the first two column X & y dataset: 0.6084\n",
      "Validation set score for the first two columns X & y dataset: 0.6130\n",
      "\n",
      "Training set score for the X small and y small dataset: 0.9348\n",
      "Validation set score for the X small and y small dataset: 0.9310\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score for X & y dataset: {:.4f}\".format(model1.score(X_train, y_train)))\n",
    "print(\"Validation set score for X & y dataset: {:.4f}\".format(model1.score(X_val, y_val)))\n",
    "print(\"\")\n",
    "print(\"Training set score for the first two column X & y dataset: {:.4f}\".format(model2.score(X_train_f2,y_train_f2)))\n",
    "print(\"Validation set score for the first two columns X & y dataset: {:.4f}\".format(model2.score(X_val_f2,y_val_f2)))\n",
    "print(\"\")\n",
    "print(\"Training set score for the X small and y small dataset: {:.4f}\".format(model3.score(X_small,y_small)))\n",
    "print(\"Validation set score for the X small and y small dataset: {:.4f}\".format(model3.score(X_small_val,y_small_val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data size  Training Accuracy  Validation Accuracy\n",
      "0     262200           0.934203             0.931304\n",
      "1       9200           0.608406             0.613043\n",
      "2      13110           0.934783             0.931034\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "results = pd.DataFrame(columns=['Data size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "results['Data size'] = [X.size, (X.iloc[:,0:2]).size, X_small.size]\n",
    "\n",
    "results['Training Accuracy'] = [model1.score(X_train, y_train),model2.score(X_train_f2,y_train_f2),model3.score(X_small,y_small) ]\n",
    "results['Validation Accuracy'] = [model1.score(X_val, y_val),model2.score(X_val_f2,y_val_f2), model3.score(X_small_val,y_small_val)]\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "Answer 1. \n",
    "\n",
    "We can see that when we have the full data with all features the accuracy score is 0.934 and validation score is 0.931. When the full dataset is available, the model has more samples to train on allowing to get greater accuracy with the complete dataset. We again see that when obtain a high validation score, which is very close to the training accuracy store. Having all the features also helps the model to find underlying patterns in the data. This is a good model.\n",
    "When only two columns are retained and rest of the data is omitted, we see see the training score drop down to 0.608 and validation score 0.613. By dropping a significant portion of the data, the model loses several key features which are important improving the accuracy of the model. The model becomes too simple and underfits the data as seen by the training and validation scores. The underlying patterns are lost when significant amount of features are dropped. This is not a good model.\n",
    "When using only 5% of the data while keeping all features, we obtain a training accuracy of 0.934 and validation accuracy 0.931. With limited amount of data, but retaining all the features in the feature matrix and having random set of data samples, we obtain good scores for the training and validation. The results from this model is good.\n",
    "\n",
    "Answer 2: A false positive is when an email is flagged as a spam when it was not a spam email. A false negative is when an email is spam but is not deleted, instead it shows up in the inbox. A false positive is worse because an important email can be flagged as spam, and you will not know about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "My process for completing this assignment was I read through the lectures slides to understand the theory component, later I reviewed the Jupyter notebooks to I sourced the code through the example jupyter notebooks provided on D2L, additionally I obtained help from ChatGPT to learn what the commands mean and how to use them.\n",
    "\n",
    "I used prompts such as explain mean squared error, how is it implemented in python, what is R2 score and how is it implemented in python, what is the meaning of model fit. These commands helped understand and do the implementation python to complete the assignment.\n",
    "\n",
    "I had challages of understanding what the commands in python were doing and what their inputs and outputs were, for example train_test_split. Through the help of ChatGPT I was able to get a better understand the use of this command and what it means.\n",
    "\n",
    "\n",
    "Citations:\n",
    "OpenAI. (2023). ChatGPT API. Retrieved from https://www.openai.com/chatgpt-api\n",
    "Dawson, Leanne. (2023). ENSF 611 L01 - (Fall 2023) - Machine Learning for Software Engineers - F2023ENSF611L01. In Desire2Learn (Brightspace). https://d2l.ucalgary.ca/d2l/home/543310\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 8)\n",
      "(1030,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "\n",
      "cement    float64\n",
      "slag      float64\n",
      "ash       float64\n",
      "water     float64\n",
      "splast    float64\n",
      "coarse    float64\n",
      "fine      float64\n",
      "age         int64\n",
      "dtype: object\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X,y = load_concrete()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "print(\"\")\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(X.isnull().sum().sum())\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Training score: 111.36\n",
      "Mean Squared Validation score: 95.90\n",
      "\n",
      "R2 Training score: 0.611\n",
      "R2 Valdiation score: 0.623\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(\"Mean Squared Training score: {:.2f}\".format(mean_squared_error(y_train, lr.predict(X_train))))\n",
    "print(\"Mean Squared Validation score: {:.2f}\".format(mean_squared_error(y_val, lr.predict(X_val))))\n",
    "print('')\n",
    "print(\"R2 Training score: {:.3f}\".format(r2_score(y_train, lr.predict(X_train))))\n",
    "print(\"R2 Valdiation score: {:.3f}\".format(r2_score(y_val, lr.predict(X_val))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>111.36</td>\n",
       "      <td>95.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 Score</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Training Accuracy  Validation accuracy\n",
       "MSE                  111.36                95.90\n",
       "R2 Score               0.61                 0.62"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "results = pd.DataFrame(columns = ['Training Accuracy', 'Validation accuracy'], index = ['MSE', 'R2 Score'])\n",
    "\n",
    "results['Training Accuracy'] = [round(mean_squared_error(y_train, lr.predict(X_train)),2),round(r2_score(y_train, lr.predict(X_train)),2)]\n",
    "results['Validation accuracy'] = [round(mean_squared_error(y_val, lr.predict(X_val)),2),round(r2_score(y_val, lr.predict(X_val)),2)]\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "Answer: based on the results provided from the mean squared error, the training MSE is 111.36 and validation MSE is 95.90. The R2 score for training is 0.61 and validation score is 0.62 which are far lower than close to 1 that we are seeking. Based on the R2 scores, the model unfits the data and can be considered too simple. It shows signs of high bias. I believe this model may not be the best fit, and its predeictions for unseen data can be unreliable. We should try another models which maybe a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "My process for completing this section of the assignment is very similar, I read through the lectures slides to understand the theory component for linear regression, later I reviewed the Jupyter notebooks provided on D2L to go through the example, additionally I obtained help from ChatGPT to learn.\n",
    "\n",
    "I used prompts on ChatGPT to get explanations on mean squared error and R2 score and how is it implemented in python, what is the meaning of model fit. The explanations on ChatGPT helped me understand and do the implementation in python to complete the assignment.\n",
    "\n",
    "I had challages of understanding what the commands in python were doing and what their inputs and outputs were, for example r2_score. Through the help of ChatGPT I was able to get a better understand the use of this command and what it means.\n",
    "\n",
    "\n",
    "Citations:\n",
    "OpenAI. (2023). ChatGPT API. Retrieved from https://www.openai.com/chatgpt-api\n",
    "Dawson, Leanne. (2023). ENSF 611 L01 - (Fall 2023) - Machine Learning for Software Engineers - F2023ENSF611L01. In Desire2Learn (Brightspace). https://d2l.ucalgary.ca/d2l/home/543310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "In part 1, with the entire spam dataset, we had a training score 0.9342 and validation score is 0.9313. The training and validation scores were very close in this case. As we lowered the number of features and used only 2 columns for the feature matrix we saw the training score drop significantly to 0.6084 and validation score to 0.6130. This is big drop from when we were using the full dataset. By using a very small dataset just the 5% we got 0.9348 as the training score and 0.9310 as the validation score. Reviewing the results, I see that we got the best results for the training and validation scores when we used the full dataset for spam. But as we drop the features, our training and validation results drop significantly. A large data sample and including all features will help us to discover underlying trends and patterns in the data.\n",
    "\n",
    "In part 2, for concrete data with linear regression, I found that model had R2 training score of 0.61 and R2 validation score to 0.62. This shows sign that the model is too simple and underfits the data. This model is not a good fit considering the results we have obtained from the training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "I liked using the python commands to develop my first machine learning model. It was exciting to work through the process and review the theory about what the results mean. I also now have a better understanding of underlying principles like training and validation data sets.\n",
    "\n",
    "I found it interesting and challenging as we try to understand how well or how bad our model works given the data. As we interpret the application of the machine learning model and assess its suitability to our data. This is a interesting and unique aspect of machine learning that I did not know about in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.611\n",
      "Validation score: 0.623\n",
      "\n",
      "Training score: 0.611\n",
      "Validation score: 0.623\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "ridge = Ridge(alpha=1).fit(X_train, y_train)\n",
    "print(\"Training score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Validation score: {:.3f}\".format(ridge.score(X_val, y_val)))\n",
    "print(\"\")\n",
    "lasso = Lasso(alpha=1).fit(X_train, y_train)\n",
    "print(\"Training score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Validation score: {:.3f}\".format(ridge.score(X_val, y_val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "The scores provided for training and validation provided by Lasso and Ridge regression were very similar after trying many different alpha values between 0.001 to 100 on the logorithmic scale. Therefore, I applied the value of alpha as 1.0 for both Ridge and Lasso models. The resulting scores from both model are not good because the training and validation scores are very low indicating the model is underfitting the data. The results are almost same as when we used Linear Regression model in the earlier part. Lasso and Ridge are both applied when we want to regularize the model to avoid overfitting, however we were not overfitting in the Linear Regression model to begin with. Lasso and Ridge are not the best models to apply in this case, perhaps other models should be tried to achieve better training and validation accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
